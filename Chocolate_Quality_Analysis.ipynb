{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chocolate Quality Analysis Using Machine Learning\n",
    "---\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/choco.jpg\"/>\n",
    "</p>\n",
    "\n",
    "This objective of this notebook is to explain how to build Machine Learning models for simple dataset such as **Sabroso Chocolate Quality Assurance** dataset.\n",
    "\n",
    "For this analysis we are going to use `Scikit-Learn`, `Pandas`, and `Numpy`.\n",
    "\n",
    "First let's import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 0.17.1\n",
      "numpy version: 1.11.3\n",
      "pandas version: 0.18.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "print('pandas version: {}'.format(pd.__version__))\n",
    "print('numpy version: {}'.format(np.__version__))\n",
    "print('pandas version: {}'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import dataset into a `Pandas` `DataFrame` and visualize that data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading data using pandas\n",
    "training = pd.read_csv('./SYNERGEN Exercise_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNIQUE_ID</th>\n",
       "      <th>LIST_OF_INGREDIENTS</th>\n",
       "      <th>PREPARATION_METHOD</th>\n",
       "      <th>MANUFACTURED_DATE</th>\n",
       "      <th>MANUFACTURED_LOCATION</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>APPETIZING_COLOR</th>\n",
       "      <th>ATTRACTIVE_PACKAGING</th>\n",
       "      <th>SUBMISSION_DATE</th>\n",
       "      <th>QUALITY_ASSURANCE_ENTITY</th>\n",
       "      <th>RESPONSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>zSmsge37tMxCUAN</td>\n",
       "      <td>X1, X2, X3, X4, X5</td>\n",
       "      <td>D2, D1, D3</td>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>Y2</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-02-02</td>\n",
       "      <td>Z1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Cd16nq5rkNVhS95</td>\n",
       "      <td>X6</td>\n",
       "      <td>D10</td>\n",
       "      <td>2017-02-02</td>\n",
       "      <td>Y1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-02-03</td>\n",
       "      <td>Z1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>NopCApkw4nCCwQw</td>\n",
       "      <td>X6</td>\n",
       "      <td>D10</td>\n",
       "      <td>2017-02-02</td>\n",
       "      <td>Y1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-02-03</td>\n",
       "      <td>Z1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>hntvWJULUiJSybI</td>\n",
       "      <td>X1, X2, X3, X4, X5</td>\n",
       "      <td>D2, D1, D3</td>\n",
       "      <td>2017-02-05</td>\n",
       "      <td>Y3</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>Z1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>w7WNrNenF4h9C5</td>\n",
       "      <td>X1, X2, X3, X4, X5</td>\n",
       "      <td>D1, D2, D3</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>Y1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Z1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           UNIQUE_ID LIST_OF_INGREDIENTS PREPARATION_METHOD MANUFACTURED_DATE  \\\n",
       "180  zSmsge37tMxCUAN  X1, X2, X3, X4, X5         D2, D1, D3        2017-02-01   \n",
       "181  Cd16nq5rkNVhS95                  X6                D10        2017-02-02   \n",
       "182  NopCApkw4nCCwQw                  X6                D10        2017-02-02   \n",
       "183  hntvWJULUiJSybI  X1, X2, X3, X4, X5         D2, D1, D3        2017-02-05   \n",
       "184   w7WNrNenF4h9C5  X1, X2, X3, X4, X5         D1, D2, D3        2017-12-31   \n",
       "\n",
       "    MANUFACTURED_LOCATION  QUANTITY  APPETIZING_COLOR  ATTRACTIVE_PACKAGING  \\\n",
       "180                    Y2       400                 0                     1   \n",
       "181                    Y1       100                 0                     1   \n",
       "182                    Y1        50                 0                     1   \n",
       "183                    Y3       150                 0                     1   \n",
       "184                    Y1        50                 1                     1   \n",
       "\n",
       "    SUBMISSION_DATE QUALITY_ASSURANCE_ENTITY  RESPONSE  \n",
       "180      2017-02-02                       Z1         0  \n",
       "181      2017-02-03                       Z1         0  \n",
       "182      2017-02-03                       Z1         0  \n",
       "183      2017-02-06                       Z1         0  \n",
       "184      2018-01-01                       Z1         0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we visualize data by just looking at first few lines\n",
    "training.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UNIQUE_ID', 'LIST_OF_INGREDIENTS', 'PREPARATION_METHOD',\n",
       "       'MANUFACTURED_DATE', 'MANUFACTURED_LOCATION', 'QUANTITY',\n",
       "       'APPETIZING_COLOR', 'ATTRACTIVE_PACKAGING', 'SUBMISSION_DATE',\n",
       "       'QUALITY_ASSURANCE_ENTITY', 'RESPONSE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And print columns\n",
    "training.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine Learning we work with tabular data (usually, load data as CSV files.) However, if we take an individual cell of **`LIST_OF_INGREDIENTS``** and **`PREPARATION_METHOD`**, each cell represent a list of properties. Hence, we need to process  and create colors to represent those properties. \n",
    "\n",
    "Firstly, we identify unique properties contains in those two columns using following simple method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Following method iterates the given pandas series\n",
    "# and find the unique properties which reside in \n",
    "# that series.\n",
    "def find_unique_items(series):\n",
    "    unique = set()\n",
    "    for elements in series.iteritems():\n",
    "        ingre = elements[1]\n",
    "        for element in ingre.split(','):\n",
    "            unique.add(element.strip())\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in LIST_OF_INGREDIENTS: {'X6', 'X10', 'X5', 'X3', 'X9', 'X1', 'X4', 'X2', 'X8', 'X7'}\n",
      "Unique values in PREPARATION_METHOD: {'D10', 'D2', 'D4', 'D1', 'D11', 'D3', 'D15', 'D5', 'D8'}\n"
     ]
    }
   ],
   "source": [
    "print('Unique values in LIST_OF_INGREDIENTS: {}'.format(find_unique_items(training['LIST_OF_INGREDIENTS'])))\n",
    "print('Unique values in PREPARATION_METHOD: {}'.format(find_unique_items(training['PREPARATION_METHOD'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, those list of properties in LIST_OF_INGREDIENTS and PREPARATION_METHOD\n",
    "# replaces with a set of columns\n",
    "def ingregient_extractor(x, ingredient):\n",
    "    for element in x.split(','):\n",
    "        y = element.strip()\n",
    "        if y == ingredient:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training['X_1'] = training['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X1',))\n",
    "training['X_2'] = training['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X2',))\n",
    "training['X_3'] = training['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X3',))\n",
    "training['X_4'] = training['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X4',))\n",
    "training['X_5'] = training['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X5',))\n",
    "training['X_6'] = training['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X6',))\n",
    "training['X_7'] = training['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X7',))\n",
    "training['X_8'] = training['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X8',))\n",
    "training['X_9'] = training['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X9',))\n",
    "training['X_10'] = training['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X10',))\n",
    "\n",
    "training['D_1'] = training['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D1',)) \n",
    "training['D_2'] = training['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D2',)) \n",
    "training['D_3'] = training['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D3',)) \n",
    "training['D_4'] = training['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D4',)) \n",
    "training['D_5'] = training['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D5',)) \n",
    "\n",
    "training['D_8'] = training['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D8',)) \n",
    "training['D_10'] = training['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D10',)) \n",
    "training['D_11'] = training['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D11',)) \n",
    "training['D_15'] = training['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D15',)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finally, we drop LIST_OF_INGREDIENTS and PREPARATION_METHOD\n",
    "del training['LIST_OF_INGREDIENTS']\n",
    "del training['PREPARATION_METHOD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RESPONSE\n",
       "0    90\n",
       "1    95\n",
       "Name: UNIQUE_ID, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.groupby('RESPONSE').count()['UNIQUE_ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data cleaning part is done, so next, we are going to encode categorical data create our training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_features = pd.concat([training[['X_1', 'X_2', 'X_3', 'X_4', 'X_5', \n",
    "                                         'X_6', 'X_7', 'X_8', 'X_9', 'X_10', \n",
    "                                         'D_1', 'D_2', 'D_3', 'D_4', 'D_5', \n",
    "                                         'D_8', 'D_10', 'D_11', 'D_15',\n",
    "                                         'QUANTITY', 'ATTRACTIVE_PACKAGING']], \n",
    "                               pd.get_dummies(training[['MANUFACTURED_LOCATION']]),\n",
    "                               pd.get_dummies(training[['QUALITY_ASSURANCE_ENTITY']])], \n",
    "                               axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model building part comes. As shown below, we trained two main Machine Learning algorithms namely: **`Logistic Regression`** and **`Random Forst`** algorithms. Also, we used 3-Fold cross validation in order to tune hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation accuracy: 0.8709677419354839\n",
      "cross validation accuracy: 0.7903225806451613\n",
      "cross validation accuracy: 0.8032786885245902\n",
      "---------------------------------------\n",
      "average corss validation accuracy: 0.821523\n",
      "CPU times: user 48 ms, sys: 0 ns, total: 48 ms\n",
      "Wall time: 47.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "X_train = training_features.values\n",
    "y_train = training['RESPONSE'].values\n",
    "\n",
    "folds = KFold(n_splits=3, shuffle=True)\n",
    "cv_accuracies = []\n",
    "for trining_idx, testing_idx in folds.split(X_train):\n",
    "    X_train_cv = X_train[trining_idx]\n",
    "    y_train_cv = y_train[trining_idx]\n",
    "    \n",
    "    X_test_cv = X_train[testing_idx]\n",
    "    y_test_cv = y_train[testing_idx]\n",
    "    \n",
    "    logistic_regression = LogisticRegression()\n",
    "    logistic_regression.fit(scale(X_train_cv), y_train_cv)\n",
    "    y_predict_cv = logistic_regression.predict(scale(X_test_cv))\n",
    "    current_accuracy = accuracy_score(y_test_cv, y_predict_cv)\n",
    "    cv_accuracies.append(current_accuracy)\n",
    "    print('cross validation accuracy: {}'.format(current_accuracy))\n",
    "    \n",
    "print( '---------------------------------------')\n",
    "print( 'average corss validation accuracy: %f' %(sum(cv_accuracies)/len(cv_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation accuracy: 0.838710\n",
      "cross validation accuracy: 0.806452\n",
      "cross validation accuracy: 0.868852\n",
      "---------------------------------------\n",
      "average corss validation accuracy: 0.838005\n",
      "---------------------------------------\n",
      "\n",
      "CPU times: user 340 ms, sys: 0 ns, total: 340 ms\n",
      "Wall time: 340 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X_train = training_features.values\n",
    "y_train = training['RESPONSE'].values\n",
    "\n",
    "folds = KFold(n_splits=3, shuffle=True)\n",
    "cv_accuracies = []\n",
    "for trining_idx, testing_idx in folds.split(X_train):\n",
    "    X_train_cv = X_train[trining_idx]\n",
    "    y_train_cv = y_train[trining_idx]\n",
    "    \n",
    "    X_test_cv = X_train[testing_idx]\n",
    "    y_test_cv = y_train[testing_idx]\n",
    "    \n",
    "    random_forest = RandomForestClassifier(n_estimators = 100)\n",
    "    random_forest.fit(scale(X_train_cv), y_train_cv)\n",
    "    y_predict_cv = random_forest.predict(scale(X_test_cv))\n",
    "    current_accuracy = accuracy_score(y_test_cv, y_predict_cv)\n",
    "    cv_accuracies.append(current_accuracy)\n",
    "    print( 'cross validation accuracy: %f' %(current_accuracy))\n",
    "\n",
    "    \n",
    "print('---------------------------------------')\n",
    "print('average corss validation accuracy: %f' %(sum(cv_accuracies)/len(cv_accuracies))) \n",
    "print( '---------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is clear that for this dataset, Random Forest works better than Logistic Regression. Hence, we take Random Forest model as our final model and going to predict response values for the testing dataset using our Random Forst model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = training_features.values\n",
    "y_train = training['RESPONSE'].values\n",
    "random_forest = RandomForestClassifier(n_estimators = 100)\n",
    "random_forest.fit(scale(X_train_cv), y_train_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, we clean the testing dataset\n",
    "testing = pd.read_csv('./SYNERGEN Exercise_Prediction.csv')\n",
    "\n",
    "testing['X_1'] = testing['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X1',))\n",
    "testing['X_2'] = testing['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X2',))\n",
    "testing['X_3'] = testing['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X3',))\n",
    "testing['X_4'] = testing['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X4',))\n",
    "testing['X_5'] = testing['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X5',))\n",
    "testing['X_6'] = testing['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X6',))\n",
    "testing['X_7'] = testing['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X7',))\n",
    "testing['X_8'] = testing['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X8',))\n",
    "testing['X_9'] = testing['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X9',))\n",
    "testing['X_10'] = testing['LIST_OF_INGREDIENTS'].apply(ingregient_extractor, args=('X10',))\n",
    "\n",
    "\n",
    "testing['D_1'] = testing['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D1',)) \n",
    "testing['D_2'] = testing['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D2',)) \n",
    "testing['D_3'] = testing['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D3',)) \n",
    "testing['D_4'] = testing['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D4',)) \n",
    "testing['D_5'] = testing['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D5',)) \n",
    "\n",
    "testing['D_8'] = testing['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D8',)) \n",
    "testing['D_10'] = testing['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D10',)) \n",
    "testing['D_11'] = testing['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D11',)) \n",
    "testing['D_15'] = testing['PREPARATION_METHOD'].apply(ingregient_extractor, args=('D15',)) \n",
    "\n",
    "del testing['LIST_OF_INGREDIENTS']\n",
    "del testing['PREPARATION_METHOD']\n",
    "\n",
    "testing_features = pd.concat([testing[['X_1', 'X_2', 'X_3', 'X_4', 'X_5', 'X_6', 'X_7', 'X_8', 'X_9', 'X_10', \n",
    "                                         'D_1', 'D_2', 'D_3', 'D_4', 'D_5', 'D_8', 'D_10', 'D_11', 'D_15',\n",
    "                                         'QUANTITY', 'ATTRACTIVE_PACKAGING']], \n",
    "                                         pd.get_dummies(testing[['MANUFACTURED_LOCATION']]),\n",
    "                                         pd.get_dummies(testing[['QUALITY_ASSURANCE_ENTITY']])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: monUvMmr95OP05e prediction: 0\n",
      "index: 1xbRcd2JbUuZ0IK prediction: 1\n",
      "index: 8FMJ6YMJYbTC4yp prediction: 0\n",
      "index: fuovowqPpCHv3W9 prediction: 0\n",
      "index: monUvMmr95OP05e prediction: 0\n",
      "index: monUvMmr95OP05e prediction: 1\n",
      "index: monUvMmr95OP05e prediction: 1\n",
      "index: monUvMmr95OP05e prediction: 0\n",
      "index: monUvMmr95OP05e prediction: 1\n",
      "index: monUvMmr95OP05e prediction: 0\n",
      "index: winhsDL92bKQS4x prediction: 1\n",
      "index: xWc5x0sOguKgkJa prediction: 0\n",
      "index: 8jE26hwkyWzOOpV prediction: 1\n",
      "index: j1KqeiH7KrzuW9N prediction: 1\n",
      "index: lBLYpZi7P5Fbs1N prediction: 0\n",
      "index: tossSzrrzX43iqu prediction: 0\n",
      "index: ZIIvtO5hcTg8Tg4 prediction: 1\n",
      "index: w7WNrNenF4h9C5 prediction: 0\n",
      "index: z8lmAhwtP3ehr63 prediction: 1\n",
      "index: QibP7kHXVqO8Ve7 prediction: 0\n",
      "index: SuNat7oTPLjWsXD prediction: 1\n",
      "index: 9KXp0XrXblr7bxy prediction: 0\n",
      "index: w7WNrNenF4h9C5 prediction: 0\n",
      "index: b8HQmdEN4W8VMfj prediction: 0\n",
      "index: Cb1ZysE3Vb0BmRc prediction: 1\n",
      "index: jbsQ6vrRg4FK2ea prediction: 1\n",
      "index: RcECLtfRYf0pIvi prediction: 0\n",
      "index: vUimrsgQNsFAl7s prediction: 0\n",
      "index: ySGmj9yzdQLoO6r prediction: 1\n",
      "index: mUxn2XVYf0BEp1n prediction: 1\n"
     ]
    }
   ],
   "source": [
    "X_test = testing_features.values\n",
    "output = random_forest.predict(X_test)\n",
    "unique_indices = training['UNIQUE_ID'].values\n",
    "for i, j in zip(output, unique_indices):\n",
    "    print('index: {} prediction: {}'.format(j, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "I would like to mention followings regarding the dataset, models and final outcome of this exercise.\n",
    "\n",
    "1. Machine Learning is all about learning from previous data. The amount of data unavailable for building the model really help to generalize the model. So more data really helps to better predictions. Unfortunately, this dataset contains less than 200 data points. Hence, building a generalize model is impossible with such a small dataset. \n",
    "\n",
    "2. Even this such a small dataset and relatively simple algorithm, we managed to achieve just above 80 percent accuracy (according to 3-Fold cross-validation) and this is much better than random guessing. (However, don't expect similar level of accuracy when it runs against testing set to the inability of the model due to small number of training example available for model building.)\n",
    "\n",
    "3. When we are building complex machine learning models one of the main concerns is managing bias-variance tradeoff. Since the simplicity of the dataset (both number of feature and number of data points),  we really restricted to simple (I would say less powerful models) models such as Logistic Regression and Random Forest. \n",
    "\n",
    "4. Finally, it was mention in the question that Java language is prepared. However, it in Machin Learning world people NEVER use Java programming language. (Yes, it is true that we have Apache Spark. But it is mainly using for large production systems.). The usual practice in Machine Learning is: First, we build quick models such as one shown above in Python or R.Next, when it comes to production we use C/C++.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
